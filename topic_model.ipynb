{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled27.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpDSTb3mJmry",
        "colab_type": "text"
      },
      "source": [
        "# What is Topic Modeling :\n",
        "\n",
        "Topic modeling is an unsupervised technique that intends to analyze large volumes of text data by clustering the documents into groups. In the case of topic modeling, the text data do not have any labels attached to it. Rather, topic modeling tries to group the documents into clusters based on similar characteristics.\n",
        "\n",
        "A typical example of topic modeling is clustering a large number of newspaper articles that belong to the same category. In other words, cluster documents that have the same topic. It is important to mention here that it is extremely difficult to evaluate the performance of topic modeling since there are no right answers. It depends upon the user to find similar characteristics between the documents of one cluster and assign it an appropriate label or topic.\n",
        "    \n",
        "Two approaches are mainly used for topic modeling: **Latent Dirichlet Allocation** and **Non-Negative Matrix factorization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoq1cMKVLOSp",
        "colab_type": "text"
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA) :\n",
        "\n",
        "**The LDA is based upon two general assumptions:**\n",
        "\n",
        "\n",
        "\n",
        "*   Documents that have similar words usually have the same topic\n",
        "*  Documents that have groups of words frequently occurring together usually have the same topic.\n",
        "\n",
        "These assumptions make sense because the documents that have the same topic, for instance, Business topics will have words like the \"economy\", \"profit\", \"the stock market\", \"loss\", etc. The second assumption states that if these words frequently occur together in multiple documents, those documents may belong to the same category.\n",
        "\n",
        "**Mathematically, the above two assumptions can be represented as:**\n",
        "\n",
        "Documents are probability distributions over latent topics\n",
        "Topics are probability distributions over words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kVmHtBEcBZq",
        "colab_type": "text"
      },
      "source": [
        "# LDA for Topic Modeling in Python:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8tCbEzjNEd",
        "colab_type": "text"
      },
      "source": [
        "In this section we will see how Python can be used to implement LDA for topic modeling. The data set can be downloaded from the Kaggle.\n",
        "\n",
        "The data set contains user reviews for different products in the food category. We will use LDA to group the user reviews into 5 categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrFP8nIBlId6",
        "colab_type": "text"
      },
      "source": [
        "**The first step, as always, is to import the data set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHusMvjcsQo",
        "colab_type": "code",
        "outputId": "02dda201-187f-4485-e931-c1064fcc3462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "\n",
        "#reviews_datasets = pd.read_csv(r'E:\\Datasets\\Reviews.csv')\n",
        "reviews_datasets = pd.read_csv(r'Reviews_c1.csv')\n",
        "reviews_datasets = reviews_datasets.head(1000)  \n",
        "reviews_datasets.dropna()\n",
        "reviews_datasets.head() "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.303862e+09</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.346976e+09</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.219018e+09</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.307923e+09</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.350778e+09</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id  ...                                               Text\n",
              "0  1.0  ...  I have bought several of the Vitality canned d...\n",
              "1  2.0  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  3.0  ...  This is a confection that has been around a fe...\n",
              "3  4.0  ...  If you are looking for the secret ingredient i...\n",
              "4  5.0  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68PYE2IbkkSH",
        "colab_type": "code",
        "outputId": "f9f77e26-f1d9-448b-92ef-10dd9f2ab83f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "reviews_datasets['Text'][350]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'These chocolate covered espresso beans are wonderful!  The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OAXmaGTk1T_",
        "colab_type": "text"
      },
      "source": [
        "**Before we can apply LDA**, we need to create vocabulary of all the words in our data. Remember from the previous article, we could do so with the help of a count vectorizer. Look at the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkesqzvkyrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')  \n",
        "doc_term_matrix = count_vect.fit_transform(reviews_datasets['Text'].values.astype('U'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyl1ibl0mAmL",
        "colab_type": "text"
      },
      "source": [
        "In the script above we use the ***CountVectorizer*** class from the ***sklearn.feature_extraction.text*** module to create a document-term matrix. We specify to only include those words that appear in less than 80% of the document and appear in at least 2 documents. We also remove all the stop words as they do not really contribute to topic modeling.\n",
        "\n",
        "**Now let's look at our document term matrix:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91D3Eb73j-xY",
        "colab_type": "code",
        "outputId": "ba3ced4d-78f3-4baa-e1cf-987a8aead96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc_term_matrix[0]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x2697 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 16 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU0Ew8SQmybv",
        "colab_type": "text"
      },
      "source": [
        "Each of 20k documents is represented as 14546 dimensional vector, which means that our vocabulary has 14546 words.\n",
        "\n",
        "**Next, we will use LDA to create topics along with the probability distribution for each word in our vocabulary for each topic:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Icefj5FmKx1",
        "colab_type": "code",
        "outputId": "43a016bb-aacc-47c5-fe2a-347a23f27397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "LDA = LatentDirichletAllocation(n_components=5, random_state=42)  \n",
        "LDA.fit(doc_term_matrix)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR6bwi_dnuwZ",
        "colab_type": "text"
      },
      "source": [
        "In the script above we use the*** LatentDirichletAllocation*** class from the ***sklearn.decomposition*** library to perform LDA on our document-term matrix. The parameter *n_components* specifies the number of categories, or topics, that we want our text to be divided into. The parameter *random_state* (aka the seed) is set to 42 so that you get the results similar to mine.\n",
        "\n",
        "Let's randomly fetch words from our vocabulary. We know that the count vectorizer contains all the words in our vocabulary. We can use the get_feature_names() method and pass it the ID of the word that we want to fetch.\n",
        "\n",
        "Let's randomly fetch words from our vocabulary. We know that the count vectorizer contains all the words in our vocabulary. We can use the get_feature_names() method and pass it the ID of the word that we want to fetch.\n",
        "\n",
        "**The following script randomly fetches 10 words from our vocabulary:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAXZwaxdnB3v",
        "colab_type": "code",
        "outputId": "e759f234-5eb0-44e6-f562-a3b082db1320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import random\n",
        "\n",
        "for i in range(10):  \n",
        "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
        "    print(count_vect.get_feature_names()[random_id])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "luckily\n",
            "vary\n",
            "handy\n",
            "8oz\n",
            "compete\n",
            "formula\n",
            "middle\n",
            "salty\n",
            "saltiness\n",
            "arrives\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7UwqIWZpS1O",
        "colab_type": "text"
      },
      "source": [
        "**Let's find 10 words with the highest probability for the first topic. To get the first topic, you can use the components_ attribute and pass a 0 index as the value:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlXtqgvqpEtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_topic = LDA.components_[0]  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbDhpxDgpYcf",
        "colab_type": "text"
      },
      "source": [
        "The first topic contains the probabilities of 14546 words for topic 1. To sort the indexes according to probability values, we can use the argsort() function. Once sorted, the 10 words with the highest probabilities will now belong to the last 10 indexes of the array. The following script \n",
        "\n",
        "**returns the indexes of the 10 words with the highest probabilities:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvxQBFkipXa3",
        "colab_type": "code",
        "outputId": "15dd4621-bae0-4dab-b9bd-9f2ee78842f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "top_topic_words = first_topic.argsort()[-10:]\n",
        "top_topic_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2440,  741, 1049,  352, 1914, 1076, 2383,  300, 1373,  963])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGF3-iHKqLdn",
        "colab_type": "text"
      },
      "source": [
        "These indexes can then be used to retrieve the value of the words from the count_vect object.\n",
        "\n",
        "**which can be done like this:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgr11JROp3t4",
        "colab_type": "code",
        "outputId": "97ef3d24-f7cf-4b17-a99b-79f5a21794c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i in top_topic_words:  \n",
        "    print(count_vect.get_feature_names()[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time\n",
            "dog\n",
            "good\n",
            "buy\n",
            "really\n",
            "great\n",
            "taste\n",
            "br\n",
            "like\n",
            "food\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGARqklpqmQi",
        "colab_type": "text"
      },
      "source": [
        "**Let's print the 10 words with highest probabilities for all the five topics:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv3PczxJqb7O",
        "colab_type": "code",
        "outputId": "0f1c0fc3-8be6-4236-c5e4-dc2b3340257b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i,topic in enumerate(LDA.components_):  \n",
        "    print(f'Top 10 words for topic #{i}:')\n",
        "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words for topic #0:\n",
            "['time', 'dog', 'good', 'buy', 'really', 'great', 'taste', 'br', 'like', 'food']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1:\n",
            "['sugar', 'price', 'taste', 'good', 'coffee', 'just', 'tea', 'product', 'like', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2:\n",
            "['taste', 'instant', 'sugar', 'mix', 'oatmeal', 'like', 'br', 'good', 'hot', 'great']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3:\n",
            "['little', 'use', 'eat', 'like', 'product', 'flavor', 'love', 'best', 'tea', 'good']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4:\n",
            "['taste', 'chip', 'salt', 'potato', 'like', 'bag', 'flavor', 'kettle', 'br', 'chips']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOv_mOoJte01",
        "colab_type": "text"
      },
      "source": [
        "The output shows that the second topic might contain reviews about chocolates, etc. Similarly, the third topic might again contain reviews about sodas or juices. You can see that there a few common words in all the categories. This is because there are few words that are used for almost all the topics. For instance \"good\", \"great\", \"like\" etc.\n",
        "\n",
        "As a final step, we will add a column to the original data frame that will store the topic for the text. To do so, we can use LDA.transform() method and pass it our document-term matrix. This method will assign the probability of all the topics to each document. \n",
        "\n",
        "**Look at the following code:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mfH2DNxqqKH",
        "colab_type": "code",
        "outputId": "1b059a33-9031-4568-ed2e-ba354bc17eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "topic_values = LDA.transform(doc_term_matrix)  \n",
        "topic_values.shape  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFvTU52Lt-iD",
        "colab_type": "text"
      },
      "source": [
        "In the output, you will see (20000, 5) which means that each of the document has 5 columns where each column corresponds to the probability value of a particular topic. To find the topic index with maximum value, we can call the argmax() method and pass 1 as the value for the axis parameter.\n",
        "\n",
        "**The following script adds a new column for topic in the data frame and assigns the topic value to each row in the column:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rakVnjDHtmIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_datasets['Topic'] = topic_values.argmax(axis=1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCS9utBsuOZy",
        "colab_type": "text"
      },
      "source": [
        "**Let's now see how the data set looks:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aqYtrC7uKAh",
        "colab_type": "code",
        "outputId": "db256294-190c-4f6d-9155-4ba1dcaa81e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "reviews_datasets.head()  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.303862e+09</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.346976e+09</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.219018e+09</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.307923e+09</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.350778e+09</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id   ProductId  ...                                               Text Topic\n",
              "0  1.0  B001E4KFG0  ...  I have bought several of the Vitality canned d...     1\n",
              "1  2.0  B00813GRG4  ...  Product arrived labeled as Jumbo Salted Peanut...     4\n",
              "2  3.0  B000LQOCH0  ...  This is a confection that has been around a fe...     3\n",
              "3  4.0  B000UA0QIQ  ...  If you are looking for the secret ingredient i...     3\n",
              "4  5.0  B006K2ZZ7K  ...  Great taffy at a great price.  There was a wid...     2\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeAmYcri4ZGL",
        "colab_type": "text"
      },
      "source": [
        "# Non-Negative Matrix Factorization (NMF):\n",
        "\n",
        "In the previous section, we saw how LDA can be used for topic modeling. In this section, we will see how non-negative matrix factorization can be used for topic modeling.\n",
        "\n",
        "Non-negative matrix factorization is also a supervised learning technique which performs clustering as well as dimensionality reduction. It can be used in combination with TF-IDF scheme to perform topic modeling. In this section, we will see how Python can be used to perform non-negative matrix factorization for topic modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0OJq1qQ4_BM",
        "colab_type": "text"
      },
      "source": [
        "**NMF for Topic Modeling in Python:**\n",
        "\n",
        "In this section, we will perform topic modeling on the same data set as we used in the last section. You will see that the steps are also quite similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNWJtl3g5D1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "77b33759-a220-42d2-8311-f15e50037ad9"
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "\n",
        "reviews_datasets = pd.read_csv(r'Reviews_c1.csv')  \n",
        "reviews_datasets = reviews_datasets.head(1000)  \n",
        "reviews_datasets.dropna()  \n",
        "reviews_datasets.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.303862e+09</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.346976e+09</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.219018e+09</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.307923e+09</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.350778e+09</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id  ...                                               Text\n",
              "0  1.0  ...  I have bought several of the Vitality canned d...\n",
              "1  2.0  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  3.0  ...  This is a confection that has been around a fe...\n",
              "3  4.0  ...  If you are looking for the secret ingredient i...\n",
              "4  5.0  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSH7vrO95tUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1279f669-68a4-45ec-d49f-5c155778a485"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')  \n",
        "doc_term_matrix = tfidf_vect.fit_transform(reviews_datasets['Text'].values.astype('U'))  \n",
        "tfidf_vect"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
              "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz-VQ_4eZnND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a386510e-c649-4b0c-f2f4-e35799ef11de"
      },
      "source": [
        "doc_term_matrix"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1000x2697 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 25469 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_TVe0EU57K7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "83a6d4ba-0b87-4dd1-f309-92f44effbec6"
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf = NMF(n_components=5, random_state=42)  \n",
        "nmf.fit(doc_term_matrix )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=5, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cffQABSo7G0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6fca5f78-8c5d-40e7-df83-e0af1bc3543c"
      },
      "source": [
        "import random\n",
        "\n",
        "for i in range(10):  \n",
        "    random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n",
        "    print(tfidf_vect.get_feature_names()[random_id])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yerba\n",
            "meaning\n",
            "amounts\n",
            "contact\n",
            "intact\n",
            "favorite\n",
            "yellow\n",
            "shots\n",
            "remember\n",
            "butterscotch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQXUhOq7Zt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07a3c726-298e-4fb2-9037-e4072e447bbb"
      },
      "source": [
        "first_topic = nmf.components_[0]  \n",
        "top_topic_words = first_topic.argsort()[-10:] \n",
        "top_topic_words"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 296, 2302,  673, 1679, 1242, 2176, 1848, 1373, 1655,  300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeLTu4bRNud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0b34fd9d-e67c-41a5-d857-2134add63361"
      },
      "source": [
        "for i in top_topic_words:  \n",
        "    print(tfidf_vect.get_feature_names()[i])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "box\n",
            "strawberry\n",
            "delicious\n",
            "pack\n",
            "ingredients\n",
            "small\n",
            "product\n",
            "like\n",
            "organic\n",
            "br\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5cN7kGARpIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "81b85aa3-0fc9-48a8-d4b5-91ca42b5894d"
      },
      "source": [
        "for i,topic in enumerate(nmf.components_):  \n",
        "    print(f'Top 10 words for topic #{i}:')\n",
        "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words for topic #0:\n",
            "['box', 'strawberry', 'delicious', 'pack', 'ingredients', 'small', 'product', 'like', 'organic', 'br']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1:\n",
            "['spicy', 'brand', 'chip', 'bag', 'flavor', 'vinegar', 'potato', 'salt', 'kettle', 'chips']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2:\n",
            "['just', 'use', 'love', 'taste', 'like', 'sugar', 'product', 'coffee', 'great', 'good']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3:\n",
            "['ahmad', 'like', 'years', 'taste', 'morning', 'black', 'drink', 'drinking', 'teas', 'tea']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4:\n",
            "['old', 'daughter', 'cats', 'best', 'foods', 'loves', 'eat', 'baby', 'dog', 'food']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNyiyaPrRqNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "2f63a86f-f97b-4ca5-c70a-ca2f82c908c5"
      },
      "source": [
        "topic_values = nmf.transform(doc_term_matrix)  \n",
        "reviews_datasets['Topic'] = topic_values.argmax(axis=1)  \n",
        "reviews_datasets.head() "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.303862e+09</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.346976e+09</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.219018e+09</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.307923e+09</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.350778e+09</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id   ProductId  ...                                               Text Topic\n",
              "0  1.0  B001E4KFG0  ...  I have bought several of the Vitality canned d...     4\n",
              "1  2.0  B00813GRG4  ...  Product arrived labeled as Jumbo Salted Peanut...     2\n",
              "2  3.0  B000LQOCH0  ...  This is a confection that has been around a fe...     2\n",
              "3  4.0  B000UA0QIQ  ...  If you are looking for the secret ingredient i...     2\n",
              "4  5.0  B006K2ZZ7K  ...  Great taffy at a great price.  There was a wid...     2\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6yNsJsHUcMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}